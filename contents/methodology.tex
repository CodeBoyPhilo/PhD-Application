\section*{Methodology}

To answer our formulated research questions and realise our objectives, we plan to conduct the research in the following steps:

\textbf{Survey on KG-augmented LLM generation methods. } 
We will continue to survey existing and emerging literature that leverages Knowledge Graph in LLM domain-specific adaptation to build a thorough understanding of the task. 
In addition, we will explore the availability of domain-specific KGs and datasets that can be used for evaluation in our research.

\textbf{Methodology formulation. }
We will research relevant information theories such as Information Bottleneck~\parencite{IB, vib, gib} to design a novel graph learning algorithm that learns a sufficiently compact knowledge representation, filtering out superfluous information while maximising the mutual information between the graph and text heterogeneous representations.
\hw{By using the knowledge representation to enrich the LLM's representation space, our method aims to provide the domain-expertise that a general-purpose LLM lacks in domain-specific QA. }
\hw{In addition, we propose to fine-tune on selective layers of the LLM to guide the LLM inference with our knowledge infused input representations.}
\hw{Thus, our method aims to mitigate LLM hallucinations in response generation without largely increasing the computational cost.}

\textbf{Evaluation on domain-specific datasets. } 
We will evaluate our proposed method on domain-specific datasets, begining with the biomedical datasets including MedQA-USMLE~\parencite{medqa} and PubMedQA~\parencite{pubmedqa}.

\textbf{Assessment of data dependency and model complexity. }
\hw{We will conduct thorough experiments and compare with existing methods to evaluate our method's reliance on the number of annotated domain-specific data and the complexity of our algorithm.}
