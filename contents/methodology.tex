\section*{Methodology}

To answer our formulated research questions and realise our objectives, we plan to conduct the research in the following steps:

\textbf{Survey on KG-augmented LLM generation methods. } 
We will continue to survey existing and emerging literature that leverages Knowledge Graph in LLM domain-specific adaptation to build a thorough understanding of the task. 
In addition, we will explore the availability of domain-specific KGs and datasets that can be used for evaluation in our research.

\textbf{Methodology formulation. }
We will design a novel knowledge representation learning algorithm that embeds a question-relevant subgraph into informative knowledge features, prevserving only useful knowledge while filtering out superfluous information. 
In addition, based on the knowledge representations learned, we will research and develop a modality fusion function that mitigates the gap between the graph and text modality, promoting a deep interaction and fusion of the two heterogeneous representation space. 
In a nut shell, our research aims to make a contribution by developing a novel learning framework for KG-augmented LLM adaptation. 

\textbf{Evaluation on domain-specific datasets. } 
We will evaluate our proposed method on domain-specific datasets and compare with baseline methods, begining with the biomedical datasets including MedQA-USMLE~\parencite{medqa} and PubMedQA~\parencite{pubmedqa}.
Through evaluation, our research aims to contribute timely new knowledge to the field by investigating how our proposed method generalises to different domains as well as the strengths and limitations of our proposed framework. 

\textbf{Assessment of the reliance on training data and sensitivity to changes in LLM architecture. }
Compared with conventional domain-specific fine-tuning, our proposed method aims to complement existing methods by achieving a lower reliance on the amounts of labeled training data used and a higher flexibility for adapting open-source LLMs to a specific domain.
Thus, we will design controled experiments, testing the model performance with varying amounts of labeled training data. 
Additionally, we will conduct experiments to assess the flexibility of our framework across various LLM architectures by altering the backbone LLM.
