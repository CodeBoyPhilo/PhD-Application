\section*{Abstract}
This proposal investigates the domain-specific adaptation of Large Language Models (LLMs) augmented with Knowledge Graphs (KGs). 
While LLMs excel at various general natural language processing tasks, they often underperform on domain-specific question answering tasks due to two key issues: \emph{hallucination} and a \emph{lack of specialised knowledge}. 
Current approaches either fine-tune LLMs with domain-specific data or utilise external resources to guide LLM inference. 
However, the former typically demands large amounts of well-annotated domain-specific data, while the latter is sensitive to both the quality of external resources and how effectively the additional information can be integrated. 
This proposal aims to enhance LLMs' domain-specific question answering ability by extracting and incorporating useful Knowledge from KGs. 
By investigating heterogeneous knowledge learning and multi-modal fusion techniques, we aim to develop a suite of novel model adaptation methods that improve LLM accuracy and reliability in domain-specific question answering. 
