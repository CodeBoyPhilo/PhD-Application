\section*{Abstract}
This proposal explores the domain-specific adaptation of Large Language Models (LLMs) augmented with Knowledge Graphs (KGs). 
While LLMs excel at various natural language processing tasks, they often underperform on domain-specific question answering tasks due to issues of \emph{hallucination} and a \emph{lack of specialised knowledge}. 
Current approaches either inject domain-specific knowledge directly into the model's parameters through fine-tuning or utilize external resources to guide LLM inference. 
However, the former typically requires large amounts of well-annotated domain-specific data, while the latter is sensitive to both the quality of external resources and how effectively the additional information can be integrated. 
This proposal aims to enhance LLMs' domain-specific question answering ability by extracting and incorporating useful Knowledge from KGs. 
By investigating information-theory-based graph learning and modality fusion algorithms, we aim to develop a suite of methods that are both data-efficient and parameter-efficient for effective LLM adaptation. 
