\section*{Abstract}
This proposal explores the domain-specific adaptation of Large Language Models (LLMs) augmented with Knowledge Graphs. 
While LLMs excel at various natural language processing tasks, they often underperform on domain-specific question answering task due to issues of \emph{hallucination} and a \emph{lack of specialised knowledge}. 
Current approaches either inject domain-specific knowledge directly into the model's parameters through fine-tuning or utilize external resources to guide LLM inference. 
However, the former typically requires large amounts of well-annotated domain-specific data, while the latter is sensitive to both the quality of external resources and how effectively the additional information is integrated. 
This proposal aims to enhance LLM domain-specific question answering ability by leveraging external Knowledge Graphs. 
By applying information-theory-based graph learning algorithms and modality fusion functions, we aim to develop a method that is both data-efficient and parameter-efficient to train. 
