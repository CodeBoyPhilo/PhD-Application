\section*{Abstract}
This proposal researches Knowledge Graph augmented Large Language Models domain-specific adaptation. 
Lagre Language Models (LLMs), although powerful with various natural language processing tasks, performs unsatisfyingly with domain-specific tasks such as Question Answering due to the issues of hallucination and lack of domain-specific expert knowledge. 
Existing approaches either directly injects new domain-specific knowledge into the model's parameters through fine-tuning, or leverages external resources to guide LLM inference. 
However, the former normally relies on large amounts of well-annotated domain-speific data and the latter is sensitive to the quality of the external resources and how the additional information is incorporated. 
This proposal aims to improve LLM domain-specific question answering by leveraging external Knowledge Graph.
Through information theory based graph learning algorithms and modality fusion function, we aim to develop method that is ultimately data-efficient and parameter-efficient to train. 
