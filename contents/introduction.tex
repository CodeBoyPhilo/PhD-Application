\section*{Introduction}\label{sec:introduction}

\hw{Large Language Models (LLMs) are generative artificial intelligence (AI) systems designed to understand, generate, and interact with human language. 
% LLMs have been attracting an increasing amount of attention not only from field experts but also from
% the public, due to their advanced reasoning capabilities and exceptional performance across a variety of natural language processing (NLP)
% tasks, including but not limited to text and code generation, summarisation, sentiment analysis, and question answering~\parencite{brown2020language, zero-shot-reasoner, palm, won2024scaling}.
With their advanced reasoning ability and intuitive interaction scheme, an increasing number of companies are incorporating LLMs into their business models to create additional value for customers. 
For instance, the fast food industry giant Domino's Pizza is leveraging LLM-based chatbots as part of their recommender system
to assist customers in placing online orders~\parencite{dominos_ai}.
Not only as customer service chatbots, generative AI technologies are believed to create a significant impact across all industry sectors, including but not limited to banking, education, legal, and healthcare.
Moreover, as a report from McKinsey \& Company points out, the implementation of LLM-based technology is expected to add more than \$3.1 trillion additional value annually, more than the GDP of the United Kingdom in 2021~\parencite{mkinsey}.}

\hw{Despite the high expectations of the LLMs in various downstream applications and specific domains, recent studies have identified two critical weaknesses that potentially impede their reliability and trustworthiness when answering domain-specific questions.}
The first critical issue is the frequent occurrence of \emph{hallucination} in LLM response generation
~\parencite{kgr, kalm-prompting, huang2023survey, ji2023survey}. 
Hallucinated responses often exhibit logical flaws and, more detrimentally, contain factually inaccurate information.
This flaw significantly impairs reasoning performance and can negatively impact the efficacy of LLMs in domain-specific question-answering (QA) tasks.
For example, consider a scenario where a user seeks biomedical advice from an LLM; receiving a response containing fabricated misinformation could potentially lead to life-threatening consequences~\parencite{pmc-llama}.
Another weakness of LLMs in domain-specific question-answering tasks is their \emph{lack of domain-specific specialised knowledge}.
A general-purpose LLMs typically possess a broad scope of information across multiple fields but lack the specialised expertise crucial for domain-specific question-answering tasks.

To combat the challenges of \emph{hallucination} and \emph{lack of specialised knowledge}, a series of methods have been recently proposed, which can be classified into domain-specific fine-tuning methods and external resources augmented methods.

\hw{Domain-specific fine-tuning methods, as the name suggests, aim to \emph{edit} the internal knowledge of the LLM by updating the model parameter through a full-scale fine-tuning or parameter-efficient fine-tuning methods such as LoRA~\parencite{lora}.
However, these fine-tuning methods often require a large number of annotated data (e.g., one million tokens) in Question-Answer pairs or Instruction-Input-Output formats. In practice, such data is often scarce due to the high annotation cost. 
In addition, the fine-tuning methods also require high computational resources as the scale of LLM parameters increases.}

\hw{On the other hand, external resources augmented methods explore the use of external resources such as databases, APIs, and Knowledge Graphs (KGs) to augment the LLM generation process. 
These methods retrieves top-$k$ relevant information from reliable sources and inject them into the LLM as contexts to enrich the response generation process.
However, these methods are susceptible to knowledge noise~\parencite{kbert}, i.e., not all retrieved knowledge is relevant to the target question.}

Very recently, research studies have attempted to investigate the use of a domain-specific Knowledge Graph (KG) as the knowledge
provider to augment LLM response generation ~\parencite{gnp, graph-prompter, kalm-prompting}.
This line of research leverages knowledge triplets to represent factual information, represented as $(e_h, r, e_t)$ where $e_h, e_t$ are the head and tail entities, and $r$ represents the relation associated with them (e.g.\ [\texttt{Canberra, CapitalOf, Australia}]).
By utilising the representation power of LLMs and relational information contained in KGs, there is considerable potential to enhance knowledge representations via information exchange between KGs and LLMs.
While these methods are less data-dependent and often model-agnostic, finding out how to best fuse the heterogeneous information between KGs and LLMs for question-answering remains an open research challenge.

Based on the above observations, we intend to research the integration between KGs and LLMs for domain-specific question-answering tasks. 
At a higher level, we tackle LLM domain-specific question-answering tasks with the objectives of achieving data efficiency (low dependence on labelled training dataset) and parameter efficiency (small-scale of trainable model parameters).
To achieve this, first, we plan to utilise context-aware graph pruning methods to remove redundant knowledge from the KGs to the maximum extent to mitigate the drawbacks of knowledge noise. 
We will further investigate the current methods of the knowledge encoder modules to formulate a graph representation learning algorithm based on information theory. 
Finally, we research methods to effectively fuse the text modality and the graph modality together to enrich the representation space of the LLM using our learned knowledge embeddings.
